# tools/scraping.py

import requests
from bs4 import BeautifulSoup
from typing import List, Dict


# ============================================================================
# SOURCES AUTORISÃ‰ES
# ============================================================================

SOURCES = [
    {
        "name": "Conservation Nature",
        "base_url": "https://www.conservation-nature.fr/plantes/"
    },
    {
        "name": "Nature & Jardin",
        "base_url": "http://nature.jardin.free.fr"
    }
]


# ============================================================================
# TOOL MCP : fetch_plant_sources
# ============================================================================

def fetch_plant_sources(query: str, limit: int = 2) -> Dict:
    """
    Tool MCP : rÃ©cupÃ¨re des informations botaniques fiables
    Ã  partir de sites spÃ©cialisÃ©s, Ã  partir dâ€™un nom de plante.

    Args:
        query (str): nom de la plante (ex: "monstera")
        limit (int): nombre maximum de sources Ã  retourner

    Returns:
        Dict contenant :
        - query
        - summary (texte rÃ©sumÃ©)
        - sources (liste de liens)
    """

    results = []
    summaries = []

    for source in SOURCES:
        if len(results) >= limit:
            break

        # Construction de lâ€™URL Ã  partir de la plante
        url = source["base_url"] + query

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # Extraction simple du texte de la page
            text = soup.get_text(separator=" ", strip=True)

            # On limite la taille pour lâ€™IA
            text = text[:1500]

            summaries.append(text)

            results.append({
                "title": source["name"],
                "url": url,
                "source_name": source["name"]
            })

        except Exception:
            # Si un site Ã©choue, on continue avec les autres
            continue

    # RÃ©sumÃ© simple (MVP) : concatÃ©nation des extraits
    summary = "\n\n".join(summaries) if summaries else None

    return {
        "query": query,
        "summary": summary,
        "sources": results
    }

# ğŸ§  Ã€ quoi sert tools/scraping.py ?

# ğŸ‘‰ Câ€™est un tool MCP
# ğŸ‘‰ Il fait une action concrÃ¨te que lâ€™IA ne peut pas faire seule

# Son rÃ´le prÃ©cis :
# aller sur un site web statique
# rÃ©cupÃ©rer des pages de plantes
# extraire du texte propre
# retourner un rÃ©sultat structurÃ© Ã  lâ€™agent IA

# ğŸ“Œ Lâ€™IA :
# ne scrape pas
# ne connaÃ®t pas le HTML
# demande simplement : â€œUtilise le tool fetch_plant_sourcesâ€

# ğŸ“Œ Le tool :
# exÃ©cute
# contrÃ´le
# retourne les donnÃ©es